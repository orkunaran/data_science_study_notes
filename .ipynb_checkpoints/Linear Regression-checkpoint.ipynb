{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "\n",
    "This is my project to learn intuition behind ML algorithms. First, I started with Linear Regression. For the OOP part I got some help from Python Engineer's youtube [video](https://www.youtube.com/watch?v=rLOyrWV8gmA). Both Gradient Descent and Cost Function are my solutions to Andrew NG Coursera Machine Learning Course Assignments. This and following project (that I will be working on) will be a study notebooks for myself, in order to remember how the algorithms work. \n",
    "\n",
    "Let's go with Linear Regression.\n",
    "\n",
    "\n",
    "# Linear Regression\n",
    "\n",
    "Linear regression (hence will be written as LR) is an modelling approach to determine the relationship between a dependent variable (y) and one or many independent variables (X).If the lenght of X equals to 1, we call this Simple or Univariate LR. If X has more variables, the process is called multiple LR.  \n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LR Hypothesis\n",
    "\n",
    "$$ h_\\theta(x) = \\theta_0 + \\theta_1x_1 $$\n",
    "\n",
    "Where;\n",
    "\n",
    "* $h_\\theta(x)$ is the hypothesis \n",
    "\n",
    "* x  is the independent variable\n",
    "\n",
    "* $ \\theta $ is the LR parameters to be learnt (note that in Multiple LR, parameters go to $\\theta_n x_n$)\n",
    "\n",
    "\n",
    "\n",
    "### Gradient Descent / derivatives\n",
    "\n",
    "Update Functions (since GD is an iterative function) :\n",
    "\n",
    "$$ w = w - \\alpha * dw $$\n",
    "\n",
    "$$ b = b - \\alpha * db $$ \n",
    "\n",
    "\n",
    "Mathematical Formulas :\n",
    "\n",
    "$ \\frac{dJ}{dw} = dw = \\frac{1}{N} \\sum_i^n -2x_i(y_i - (wx_i + b)) \n",
    "                     = \\frac{1}{N} \\sum_{i=1}^n2x_i(\\hat y - y_i) $\n",
    "                     \n",
    "$ \\frac{dJ}{db} = db = \\frac{1}{N} \\sum_i^n -2(y_i - (wx_i + b)) \n",
    "                     = \\frac{1}{N} \\sum_{i=1}^n2(\\hat y - y_i) $\n",
    "                     \n",
    "                                         \n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cost Function and Gradient Descent\n",
    "\n",
    "Cost Function (aka Loss , Error Function) is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event. An optimization problem seeks to minimize a loss function.\n",
    "\n",
    "The objective of linear regression is to minimize the cost function (error function or loss function) which means getting predictions as close as real values.\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2$$\n",
    "\n",
    "Recall that the parameters of your model are the $\\theta_j$ values. These are\n",
    "the values you will adjust to minimize cost $J(\\theta)$. One way to do this is to\n",
    "use the batch gradient descent algorithm. In batch gradient descent, each\n",
    "iteration performs the update\n",
    "\n",
    "$$ \\theta_j = \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)x_j^{(i)} \\qquad \\text{simultaneously update } \\theta_j \\text{ for all } j$$\n",
    "\n",
    "With each step of gradient descent, your parameters $\\theta_j$ come closer to the optimal values that will achieve the lowest cost J($\\theta$)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "import numpy as np\n",
    "\n",
    "class LinRegression():\n",
    "    def __init__(self, learning_rate = 0.001, n_iters = 1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.weights = None #coefficients\n",
    "        self.bias = None # bias \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "        self.bias = 0\n",
    "        \n",
    "        # derivatives for gradient descent\n",
    "        for _ in range(self.n_iters):\n",
    "            y_prediction = np.dot(X, self.weights) + self.bias\n",
    "            \n",
    "            dw = (1/len(X)) * np.dot(X.T, y_prediction - y)\n",
    "            db = (1/len(X)) * np.sum(y_prediction - y)\n",
    "            \n",
    "            self.weights -= self.learning_rate  * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_prediction = np.dot(X, self.weights) + self.bias\n",
    "        return y_prediction"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "# Cost Function ; this code is a part of Andrew NG's coursera assignments\n",
    "\n",
    "def cost_function(X,y,theta):\n",
    "    \"\"\" Computes Cost Function for linear regression. \n",
    "    Computes the loss function using theta parameters\n",
    "    for linear regression to fit data points X and y.\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X = array-like\n",
    "        The input array, (m, n+1) shape, where m is the number of datapoints (rows), \n",
    "        n is the number of features (columns)\n",
    "    y = array-like\n",
    "        The target vector with the (m,) shape.\n",
    "    \n",
    "    theta = array- like\n",
    "        Parameters for regression, (n+1,) shape\n",
    "    \n",
    "    \"\"\"\n",
    "    J = 0 # this is the cost function, at the end, code will return this\n",
    "    \n",
    "    m = y.shape[0] \n",
    "    \n",
    "    J = np.sum((np.dot(X, theta) - y)**2)/(2*m)\n",
    "    \n",
    "    return J\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "# Gradient Descent, this one also a part of Andrew NG's coursera assignment\n",
    "\n",
    "def GradientDescent(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to learn `theta`. Updates theta by taking `num_iters`\n",
    "    gradient steps with learning rate `alpha`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like\n",
    "        The input dataset of shape (m x n+1).\n",
    "    \n",
    "    y : array_like\n",
    "        Value at given features. A vector of shape (m, ).\n",
    "    \n",
    "    theta : array_like\n",
    "        Initial values for the linear regression parameters. \n",
    "        A vector of shape (n+1, ).\n",
    "    \n",
    "    alpha : float\n",
    "        The learning rate.\n",
    "    \n",
    "    num_iters : int\n",
    "        The number of iterations for gradient descent. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    theta : array_like\n",
    "        The learned linear regression parameters. A vector of shape (n+1, ).\n",
    "    \n",
    "    J_history : list\n",
    "        A python list for the values of the cost function after each iteration.\n",
    "    \n",
    "    Instructions\n",
    "    ------------\n",
    "    Peform a single gradient step on the parameter vector theta.\n",
    "\n",
    "    While debugging, it can be useful to print out the values of \n",
    "    the cost function (computeCost) and gradient here.\n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    m = y.shape[0]  # number of training examples\n",
    "    \n",
    "    # make a copy of theta, to avoid changing the original array, since numpy arrays\n",
    "    # are passed by reference to functions\n",
    "    theta = theta.copy()\n",
    "    \n",
    "    J_history = [] # Use a python list to save cost in every iteration\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        \n",
    "        theta = theta - (alpha / m)* (np.dot(X, theta) - y).dot(X)\n",
    "        \n",
    "        # save the cost J in every iteration\n",
    "        J_history.append(cost_function(X, y, theta))\n",
    "    \n",
    "    return theta, J_history"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "# Testing"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X,y, coef = datasets.make_regression(n_samples=100, n_features=1,\n",
    "                                      n_informative=1, noise=10,\n",
    "                                      coef=True, random_state=0)\n",
    "\n",
    "X_train = X[:70]\n",
    "X_test = X[70:]\n",
    "y_train = y[:70]\n",
    "y_test = y[70:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train,y_train)\n",
    "y_pred = lin_reg.predict(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "lr = LinRegression(learning_rate=0.1)\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred_self = lr.predict(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "r2_score(y_test,y_pred), r2_score(y_pred_self, y_test)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(0.9339148813115155, 0.94305198251888)"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "theta = [0.001, 0.005, 0.01, 0.1, 1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "for x in theta:\n",
    "    lr = LinRegression(learning_rate = x)\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred_self = lr.predict(X_test)\n",
    "    print(x, ':', r2_score(y_pred_self, y_test))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.001 : 0.753036416960501\n",
      "0.005 : 0.9436957652068035\n",
      "0.01 : 0.9430581802945025\n",
      "0.1 : 0.94305198251888\n",
      "1 : 0.9430519825188799\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "for x in theta:\n",
    "    print(x, ':', cost_function(X_train,y_train,x))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.001 : 77270.21077245871\n",
      "0.005 : 77270.07250460779\n",
      "0.01 : 77269.90139478355\n",
      "0.1 : 77267.14916593912\n",
      "1 : 77273.78166822693\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "98ed295b1837f6757218532b081f1846f69a0a775ce5d41147ef6add03ed034c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}