{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c2bb9d",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is my project to learn intuition behind ML algorithms. First, I started with Linear Regression. Both Gradient Descent and Cost Function are my solutions to Andrew NG Coursera Machine Learning Course Assignments. This and following project (that I will be working on) will be a study notebooks for myself, in order to remember how the algorithms work. \n",
    "\n",
    "Let's go with Linear Regression.\n",
    "\n",
    "\n",
    "# Linear Regression\n",
    "\n",
    "Linear regression (hence will be written as LR) is an modelling approach to determine the relationship between a dependent variable (y) and one or many independent variables (X).If the lenght of X equals to 1, we call this Simple or Univariate LR. If X has more variables, the process is called multiple LR.  \n",
    "\n",
    "Before going deep into LR functions, we need to know assumptions of LR. Regression has 5 assumptions:\n",
    "\n",
    "1. Linear Relation : The relationship between dependent and independent variable must be linear. \n",
    "2. Multivariate Normality: LR also requires all variables to be normally distributed. For normal distribution check Q-Q plots, histograms, Kolomogrov-Smirnov test, skewness and kurtosis values. \n",
    "3. No Multicollinearity : LR requires independent variables not correlated with each other. To determine multicollinearity, Variance Inflation Factor value, correlation matrix (sns.heatmap(df.corr()) or tolerance.  If multicollinearity present in the data, we should try removing one of the variables or create a new feature.\n",
    "    - the tolerance measures the influence of one independent variable on all other independent variables; the tolerance is calculated with an initial linear regression analysis.  Tolerance is defined as T = 1 â€“ RÂ² for these first step regression analysis.  With T < 0.1 there might be multicollinearity in the data and with T < 0.01 there certainly is.\n",
    "    - the variance inflation factor of the linear regression is defined as VIF = 1/T. With VIF > 5 there is an indication that multicollinearity may be present; with VIF > 10 there is certainly multicollinearity among the variables.\n",
    "    \n",
    "4. No autocorrelation : Autocorrelation occurs when the residuals are not independent from each other.  In other words when the value of y(x+1) is not independent from the value of y(x).  If autocorrelation present, we might not be sure which variable estimates the dependent variable better than each other. Durbin-Watson d score is used to determine autocorrelation of the data. d scores around 2 is accepted as no autocorrelation. 1.5<d<2.5 (ranging 0 to 4) is assumed that data has autocorrelation.\n",
    "\n",
    "5. Homoscedasticity : LR requires residuals near the regression line. The best way to evaluate that sns.regplot or scatter plots.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6d4289",
   "metadata": {},
   "source": [
    "### LR Hypothesis\n",
    "\n",
    "$$ h_\\theta(x) = \\theta_0 + \\theta_1x_1 $$\n",
    "\n",
    "Where;\n",
    "\n",
    "* $h_\\theta(x)$ is the hypothesis \n",
    "\n",
    "* x  is the independent variable\n",
    "\n",
    "* $Â \\theta $ is the modelâ€™s parameter vector, containing the bias term $Â \\theta_0 $ and the feature weights $Â \\theta_1 $ to $ \\theta_n$.\n",
    "\n",
    "### Performance Measures\n",
    "\n",
    "To determine how well or poorly LR model trained we need to calculate an error value. For LR we have Mean Squared Error, Root Mean Squared Error. To calculate MSE of given X is :\n",
    "\n",
    "$$ MSE(X, h_{\\theta}) = \\frac{1}{m}\\sum_{i=1}^m(\\theta^Tx^i-y^i)^2Â $$\n",
    "\n",
    "\n",
    "#### Normal Equation\n",
    "\n",
    "To find the $\\theta$ value that minimize cost function, we use normal equation formula:\n",
    "\n",
    "$$ \\hat{\\theta} = (X^TX)^{-1}X^Ty$$\n",
    "                     \n",
    "                                         \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72fa869",
   "metadata": {},
   "source": [
    "### Cost Function and Gradient Descent\n",
    "\n",
    "Cost Function (aka Loss , Error Function) is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event. An optimization problem seeks to minimize a loss function. Gradient Descent algorithm is used for optimizing cost function. \n",
    "\n",
    "The common example for GD is climbing down a foggy mountain. You can only feel the slope, but cannot see where you are going. The easiest way to go down, take the steepest slope. That what GD does for LR.\n",
    "\n",
    "First, take a random ðœƒ value (random initialization), improve it slowly (learning_rate) to find global (or local) minima. Learning rate is how big or small your step is. A big learning rate may cause you to skip minimum, or a small learning rate might take too much time.\n",
    "\n",
    "Gradient Descent Formula:\n",
    "\n",
    "$$ \\frac{dJ}{dw} = \\frac{1}{m}\\sum_{i=1}^m(\\theta^Tx^{(i)}-y^{(i)})x_j^{i} $$\n",
    "\n",
    "Update Functions (since GD is an iterative function) :\n",
    "\n",
    "$$ w = w - \\alpha * dw $$\n",
    "\n",
    "$$ b = b - \\alpha * db $$ \n",
    "\n",
    "\n",
    "Mathematical Formulas :\n",
    "\n",
    "$$ \\frac{dJ}{dw} = dw = \\frac{1}{N} \\sum_{i=1}^n2x_i(\\hat y - y_i) $$\n",
    "                     \n",
    "$$ \\frac{dJ}{db} = db = \\frac{1}{N} \\sum_{i=1}^n2(\\hat y - y_i) $$\n",
    "\n",
    "#### Batch Gradient Descent\n",
    "\n",
    "In Batch Gradient Descent, all the training data is taken into consideration to take a single step. We take the average of the gradients of all the training examples and then use that mean gradient to update our parameters. Batch Gradient Descent is great for convex or relatively smooth error manifolds. In this case, we move somewhat directly towards an optimum solution.\n",
    "\n",
    "Partial Derivatives for Cost Function: \n",
    "$$\\frac{\\partial}{\\partial\\theta_j}MSE(\\theta) = \\frac{2}{m}\\sum_{i=1}^m(\\theta^Tx^{(i)}-y^{(i)})x_j^{i}$$\n",
    "\n",
    "\n",
    "Gradinet Vector of the Cost Function: \n",
    "$$ = \\frac{2}{m} X^T(X\\theta-y) $$\n",
    "\n",
    "\n",
    "#### Stochastic Gradient Descent\n",
    "\n",
    "The main problem for Batch Gradient Descent is that it uses all the data for determining global minima. However this is problematic for large datasets cause of the computation durations. \n",
    "\n",
    "On the other hand Stochastic Gradient Descent picks random instance from the data and computes the gradient based on only one instance. This makes the S-GD way more faster and ram efficient (only used instance is saved to ram).\n",
    "\n",
    "S-GD due its stochastic (random) nature, is less regular than Batch GD. S-GD moves up and down and gently moves down to global minima ( at least nearly). However, it never stops and tries to move around a bit more. Overall, it will end up close to minimum but not exactly. \n",
    "But this specification has one positive outcome. When to cost function is irregular and has local minima's, S-GD is more likely to find Global Minima, and it is considered better than Batch Gradient Descent. \n",
    "\n",
    "As mentioned above, randomness is good to escape local optima, however it is bad that it can never settle at the minimum. To avoid that, we need to decrease the learning rate gradually. First start with a big learning rate to jump out of the starting point and local minima, than gradually decrease the learnin rate to allow the algorithm settle at the global minima. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "9400ac36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normal_equation(X,y):\n",
    "    \"\"\" Calculates normal equation for linear regression. \n",
    "    Normal equation finds the value that minimizes theta.\n",
    "    theta[0] is intercept, theta[1] is coef_\n",
    "    \"\"\"\n",
    "    X_lr = np.concatenate((np.ones((len(X),1)),X),axis=1)\n",
    "    global theta\n",
    "    theta = np.linalg.pinv(X_lr.T.dot(X_lr)).dot(X_lr.T).dot(y)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d0da3c-1c4d-4d2e-a06f-f6142b12df1c",
   "metadata": {},
   "source": [
    "Scikit_learn LR uses scipy.linalg.lstsq() function. and it uses pseudoinverse of matrix $X^+$. Let's see how O'reilly book defines this situation:\n",
    "\n",
    "- The pseudoinverse itself is computed using a standard matrix factorization technique called Singular Value Decomposition (SVD) that can decompose the training set matrix X into the matrix multiplication of three matrices U Î£ VT (see numpy.linalg.svd()). The pseudoinverse is computed as X+ = VÎ£+UT. To compute the matrix Î£+, the algorithm takes Î£ and sets to zero all values smaller than a tiny threshold value, then it replaces all the non-zero values with their inverse, and finally it transposes the resulting matrix. This approach is more efficient than computing the Normal Equation, plus it handles edge cases nicely: indeed, the Normal Equation may not work if the matrix XTX is not invertible (i.e., singular), such as if m < n or if some features are redundant, but the pseudoinverse is always defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "72cddfde-dca4-4374-acea-8f8c20b89147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(X, theta):\n",
    "    \"\"\"Predicts y with given X,\n",
    "    do not forget to calculate theta with normal_equation\n",
    "    \"\"\"\n",
    "    x_new = np.concatenate((np.ones((len(X),1)),X),axis=1) #add ones (x0) to X\n",
    "    y_pred = x_new.dot(theta)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ddf40a39-8a01-4526-a8d7-459bb53ade1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_pred,y):\n",
    "    \"\"\"\n",
    "    Calculates mean squared error\n",
    "    \"\"\"\n",
    "    m = y_pred.shape\n",
    "    mse = pow(np.sum(y- y_pred),2)/m\n",
    "    return mse\n",
    "\n",
    "def rmse(mse):\n",
    "    \"\"\"\n",
    "    Calculates Root mean squared error\n",
    "    \"\"\"\n",
    "    return np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "b302db35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost Function ; this code is a part of Andrew NG's coursera assignments\n",
    "\n",
    "def cost_function(X,y,theta):\n",
    "    \"\"\" Computes Cost Function for linear regression. \n",
    "    Computes the loss function using theta parameters\n",
    "    for linear regression to fit data points X and y.    \n",
    "    \"\"\"\n",
    "    J = 0 # this is the cost function, at the end, code will return this\n",
    "    \n",
    "    m = y.shape[0] \n",
    "    \n",
    "    X = np.concatenate((np.ones((len(X),1)),X),axis=1) #add ones (x0) to X\n",
    "    \n",
    "    J = np.sum((np.dot(X, theta) - y)**2)/(2*m)\n",
    "    \n",
    "    return J\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "af1a8d82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Â Gradient Descent, this one also a part of Andrew NG's coursera assignment\n",
    "\n",
    "def GradientDescent(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Performs gradient descent to learn `theta`. Updates theta by taking `num_iters`\n",
    "    gradient steps with learning rate `alpha`.\n",
    "    \"\"\"\n",
    "    # Initialize some useful values\n",
    "    m = y.shape[0]  # number of training examples\n",
    "    \n",
    "    # make a copy of theta, to avoid changing the original array, since numpy arrays\n",
    "    # are passed by reference to functions\n",
    "    theta = theta.copy()\n",
    "    \n",
    "    X = np.concatenate((np.ones((len(X),1)),X),axis=1) #add ones (x0) to X\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        \n",
    "        gredients = (1/m) * X.T.dot(X.dot(theta) - y)\n",
    "        \n",
    "        theta = theta - alpha * gredients\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "bcbdcc46-8196-4636-b25c-42ce6e3f2770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient(X,y, theta, learning_rate = 0.01, num_iters = 1000):\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    X = np.concatenate((np.ones((len(X),1)),X),axis=1) #add ones (x0) to X\n",
    "    \n",
    "    J_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        \n",
    "        gradients = (2/m) * X.T.dot(X.dot(theta) - y)\n",
    "        \n",
    "        theta -= learning_rate * gradients\n",
    "        \n",
    "        J_history.append(gradients)\n",
    "    \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "cbdb2968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "f2d9ce4e-7e33-4dd3-8449-607d79f72bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.7528377 ],\n",
       "       [3.12961683]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = normal_equation(X,y)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "833c3852-377e-467c-8cde-1b666780d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = prediction(np.array([[2],[1]]), theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "53487647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.638378355975523"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost_function(X,y,theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "e2d97e1b-7a08-486f-b8ff-cbf514d21a03",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.7528377 ],\n",
       "       [3.12961683]])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GradientDescent(X, y, theta=theta, alpha= 0.1, num_iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "0627d4d5-9799-4f82-909d-6bbea186be1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.7528377 ],\n",
       "       [3.12961683]])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_gradient(X, y, theta=theta)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98ed295b1837f6757218532b081f1846f69a0a775ce5d41147ef6add03ed034c"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
